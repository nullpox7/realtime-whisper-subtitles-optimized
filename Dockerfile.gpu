# Real-time Whisper Subtitles - GPU Optimized Dockerfile (v2.2.0)
# NVIDIA CUDA 12.9 + cuDNN optimized for large-v3 model maximum accuracy
# Author: Real-time Whisper Subtitles Team
# Encoding: UTF-8

FROM nvidia/cuda:12.9-devel-ubuntu22.04

# Set environment variables
ENV DEBIAN_FRONTEND=noninteractive
ENV PYTHONUNBUFFERED=1
ENV PYTHONDONTWRITEBYTECODE=1
ENV PIP_NO_CACHE_DIR=1
ENV PIP_DISABLE_PIP_VERSION_CHECK=1
ENV LOG_PATH=/app/data/logs

# CUDA 12.9 and GPU optimization environment variables
ENV CUDA_VISIBLE_DEVICES=all
ENV NVIDIA_VISIBLE_DEVICES=all
ENV NVIDIA_DRIVER_CAPABILITIES=compute,utility
ENV CUDA_CACHE_PATH=/tmp/cuda_cache
ENV TORCH_CUDA_ARCH_LIST="7.0;7.5;8.0;8.6;8.9;9.0"

# Numba optimization for GPU (enable JIT with proper caching)
ENV NUMBA_DISABLE_JIT=0
ENV NUMBA_CACHE_DIR=/tmp/numba_cache
ENV NUMBA_THREADING_LAYER=workqueue
ENV NUMBA_PARALLEL=1

# CUDA 12.9 specific optimizations
ENV CUDA_MODULE_LOADING=LAZY
ENV CUDA_DEVICE_ORDER=PCI_BUS_ID
ENV TORCH_CUDNN_V8_API_ENABLED=1

# Install system dependencies
RUN apt-get update && apt-get install -y \
    # Build tools
    build-essential \
    cmake \
    git \
    curl \
    wget \
    pkg-config \
    # Python 3.11
    python3.11 \
    python3.11-dev \
    python3-pip \
    # Audio processing
    ffmpeg \
    libavcodec-dev \
    libavformat-dev \
    libavutil-dev \
    libswscale-dev \
    libswresample-dev \
    libasound2-dev \
    portaudio19-dev \
    libportaudio2 \
    libportaudiocpp0 \
    libpulse-dev \
    # Additional tools
    htop \
    nvtop \
    # CUDA toolkit components
    libnccl2 \
    libnccl-dev \
    && rm -rf /var/lib/apt/lists/*

# Create symbolic link for python
RUN ln -s /usr/bin/python3.11 /usr/bin/python3 && \
    ln -s /usr/bin/python3.11 /usr/bin/python

# Create app user for security
RUN groupadd -r appuser && useradd -r -g appuser -s /bin/bash appuser

# Set working directory
WORKDIR /app

# Copy requirements file first for better caching
COPY requirements.gpu.txt .

# Upgrade pip and install wheel
RUN python3 -m pip install --upgrade pip setuptools wheel

# Install PyTorch with CUDA 12.9 support (latest versions)
RUN python3 -m pip install torch==2.3.1+cu121 torchaudio==2.3.1+cu121 --index-url https://download.pytorch.org/whl/cu121

# Install other Python dependencies
RUN python3 -m pip install --no-cache-dir -r requirements.gpu.txt

# Create necessary directories with proper permissions
RUN mkdir -p /app/data/{models,outputs,logs,cache} \
    && mkdir -p /app/static \
    && mkdir -p /app/templates \
    && mkdir -p /app/src \
    && mkdir -p /app/config \
    && mkdir -p /app/logs \
    && mkdir -p /tmp/cuda_cache \
    && mkdir -p /tmp/numba_cache \
    && mkdir -p /home/appuser/.cache \
    && mkdir -p /home/appuser/.torch \
    && chown -R appuser:appuser /app \
    && chown -R appuser:appuser /home/appuser \
    && chown -R appuser:appuser /tmp/cuda_cache \
    && chown -R appuser:appuser /tmp/numba_cache \
    && chmod -R 755 /tmp/cuda_cache \
    && chmod -R 755 /tmp/numba_cache \
    && chmod -R 755 /home/appuser

# Copy application files
COPY src/ ./src/
COPY static/ ./static/
COPY templates/ ./templates/

# Copy .env.gpu.example if it exists
COPY .env.gpu.example* ./

# Set final permissions
RUN chown -R appuser:appuser /app

# Switch to non-root user
USER appuser

# Create user-specific data directories
RUN mkdir -p /app/data/{models/whisper,outputs,logs,cache} \
    && mkdir -p /app/logs \
    && mkdir -p /home/appuser/.cache/torch \
    && mkdir -p /home/appuser/.cache/huggingface

# Pre-download large-v3 model (optional but recommended)
RUN python3 -c "
try:
    from faster_whisper import WhisperModel
    print('Pre-downloading large-v3 model...')
    model = WhisperModel('large-v3', device='cpu', download_root='/app/data/models/whisper')
    print('Model downloaded successfully!')
except Exception as e:
    print(f'Model download failed: {e}')
    print('Will download on first use')
"

# Expose port
EXPOSE 8000

# Health check with longer timeout for GPU initialization
HEALTHCHECK --interval=30s --timeout=60s --start-period=180s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# Set default environment variables optimized for CUDA 12.9 and large-v3
ENV HOST=0.0.0.0
ENV PORT=8000
ENV WHISPER_MODEL=large-v3
ENV LANGUAGE=auto
ENV DEVICE=cuda
ENV COMPUTE_TYPE=float16
ENV BEAM_SIZE=5
ENV TEMPERATURE=0.0
ENV BEST_OF=5
ENV ENABLE_WORD_TIMESTAMPS=true
ENV VAD_FILTER=true
ENV BATCH_SIZE=16
ENV MAX_WORKERS=4

# CUDA 12.9 specific memory optimizations
ENV PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
ENV CUDA_MEMORY_FRACTION=0.85

# Default command optimized for CUDA 12.9
CMD ["python3", "-m", "uvicorn", "src.web_interface:app", "--host", "0.0.0.0", "--port", "8000", "--workers", "1"]
# Real-time Whisper Subtitles - GPU Working Environment (Fixed)
# CUDA 12.4 optimized configuration for guaranteed compatibility
# Copy this file to .env and modify as needed
# Encoding: UTF-8

# ========================================
# Server Configuration
# ========================================
HOST=0.0.0.0
PORT=8000
DEBUG=false

# ========================================
# Whisper Model Configuration (Stable GPU Settings)
# ========================================
WHISPER_MODEL=large-v3
LANGUAGE=auto

# ========================================
# Audio Processing Configuration
# ========================================
SAMPLE_RATE=16000
CHUNK_SIZE=1024
VAD_MODE=3

# ========================================
# GPU Performance Configuration (CUDA 12.4 Optimized)
# ========================================
DEVICE=cuda
COMPUTE_TYPE=float16
MAX_WORKERS=4
BATCH_SIZE=16

# ========================================
# High Accuracy Processing Configuration
# ========================================
BEAM_SIZE=5
BEST_OF=5
TEMPERATURE=0.0
LENGTH_PENALTY=1.0
PATIENCE=1.0

# ========================================
# Advanced Transcription Settings
# ========================================
ENABLE_WORD_TIMESTAMPS=true
VAD_FILTER=true
COMPRESSION_RATIO_THRESHOLD=2.4
LOG_PROB_THRESHOLD=-1.0
NO_SPEECH_THRESHOLD=0.6
CONDITION_ON_PREVIOUS_TEXT=false

# ========================================
# CUDA 12.4 GPU Optimization
# ========================================
CUDA_VISIBLE_DEVICES=all
NVIDIA_VISIBLE_DEVICES=all
NVIDIA_DRIVER_CAPABILITIES=compute,utility
CUDA_CACHE_PATH=/tmp/cuda_cache
TORCH_CUDA_ARCH_LIST=7.0;7.5;8.0;8.6;8.9;9.0

# ========================================
# GPU Memory Management (CUDA 12.4 Stable)
# ========================================
CUDA_MEMORY_FRACTION=0.85
PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
CUDA_MODULE_LOADING=LAZY
TORCH_CUDNN_V8_API_ENABLED=1

# ========================================
# Numba Configuration (GPU Optimized)
# ========================================
NUMBA_DISABLE_JIT=0
NUMBA_CACHE_DIR=/tmp/numba_cache
NUMBA_THREADING_LAYER=workqueue
NUMBA_PARALLEL=1

# ========================================
# Paths Configuration
# ========================================
MODEL_PATH=/app/data/models
OUTPUT_PATH=/app/data/outputs
LOG_PATH=/app/data/logs

# ========================================
# Logging Configuration
# ========================================
LOG_LEVEL=INFO

# ========================================
# Security Configuration
# ========================================
RATE_LIMIT=100
MAX_UPLOAD_SIZE=100

# ========================================
# CORS Configuration
# ========================================
ENABLE_CORS=true
CORS_ORIGINS=*

# ========================================
# Localization Configuration
# ========================================
TZ=Asia/Tokyo
LANG=en_US.UTF-8
LC_ALL=en_US.UTF-8

# ========================================
# Docker Configuration
# ========================================
DOCKER_NETWORK=whisper-network

# ========================================
# Performance Tuning by GPU Type
# ========================================
# Uncomment and adjust based on your GPU:

# For RTX 4090/4080 (24GB/16GB VRAM)
# BATCH_SIZE=32
# MAX_WORKERS=6
# CUDA_MEMORY_FRACTION=0.9

# For RTX 4070/3080 (12GB VRAM) - Recommended settings
# BATCH_SIZE=16
# MAX_WORKERS=4
# CUDA_MEMORY_FRACTION=0.85

# For RTX 3060/4060 (8GB VRAM)
# BATCH_SIZE=8
# MAX_WORKERS=2
# CUDA_MEMORY_FRACTION=0.8

# For GTX 1660 Ti/2060 (6GB VRAM)
# BATCH_SIZE=4
# MAX_WORKERS=1
# CUDA_MEMORY_FRACTION=0.75
# WHISPER_MODEL=medium  # Use smaller model

# ========================================
# Model Size Guide
# ========================================
# tiny (39MB)    - Fastest, lowest quality, good for testing
# base (74MB)    - Balanced speed and quality
# small (244MB)  - Good quality, moderate speed
# medium (769MB) - High quality, needs more VRAM
# large-v2 (1.5GB) - Very high quality, 6GB+ VRAM
# large-v3 (1.5GB) - Best quality, 8GB+ VRAM recommended
# Real-time Whisper Subtitles - GPU Optimized Docker Compose (v2.2.3)
# NVIDIA CUDA 12.4.1 + large-v3 model configuration for maximum accuracy
# Encoding: UTF-8

services:
  # Main Whisper Application (GPU Optimized for CUDA 12.4.1)
  whisper-subtitles-gpu:
    build:
      context: .
      dockerfile: Dockerfile.gpu
    restart: unless-stopped
    ports:
      - "${PORT:-8000}:8000"
    volumes:
      - ./data/models:/app/data/models
      - ./data/outputs:/app/data/outputs
      - ./data/logs:/app/data/logs
      - ./data/cache:/app/data/cache
      # GPU cache volumes for persistence
      - gpu_model_cache:/home/appuser/.cache/torch
      - gpu_huggingface_cache:/home/appuser/.cache/huggingface
    environment:
      - HOST=${HOST:-0.0.0.0}
      - PORT=${PORT:-8000}
      - DEBUG=${DEBUG:-false}
      # Large-v3 model configuration (maximum accuracy)
      - WHISPER_MODEL=${WHISPER_MODEL:-large-v3}
      - LANGUAGE=${LANGUAGE:-auto}
      - SAMPLE_RATE=${SAMPLE_RATE:-16000}
      - CHUNK_SIZE=${CHUNK_SIZE:-1024}
      # CUDA 12.4.1 optimization settings
      - DEVICE=cuda
      - COMPUTE_TYPE=float16
      - BEAM_SIZE=${BEAM_SIZE:-5}
      - BEST_OF=${BEST_OF:-5}
      - TEMPERATURE=${TEMPERATURE:-0.0}
      - LENGTH_PENALTY=${LENGTH_PENALTY:-1.0}
      - PATIENCE=${PATIENCE:-1.0}
      # High accuracy processing settings
      - ENABLE_WORD_TIMESTAMPS=${ENABLE_WORD_TIMESTAMPS:-true}
      - VAD_FILTER=${VAD_FILTER:-true}
      - COMPRESSION_RATIO_THRESHOLD=${COMPRESSION_RATIO_THRESHOLD:-2.4}
      - LOG_PROB_THRESHOLD=${LOG_PROB_THRESHOLD:--1.0}
      - NO_SPEECH_THRESHOLD=${NO_SPEECH_THRESHOLD:-0.6}
      # Performance optimization (CUDA 12.4.1)
      - MAX_WORKERS=${MAX_WORKERS:-4}
      - BATCH_SIZE=${BATCH_SIZE:-16}
      - MODEL_PATH=/app/data/models
      - OUTPUT_PATH=/app/data/outputs
      - LOG_PATH=/app/data/logs
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      # API settings
      - RATE_LIMIT=${RATE_LIMIT:-100}
      - MAX_UPLOAD_SIZE=${MAX_UPLOAD_SIZE:-100}
      - ENABLE_CORS=${ENABLE_CORS:-true}
      - CORS_ORIGINS=${CORS_ORIGINS:-*}
      # Localization
      - TZ=${TZ:-Asia/Tokyo}
      - LANG=${LANG:-en_US.UTF-8}
      - LC_ALL=${LC_ALL:-en_US.UTF-8}
      # CUDA 12.4.1 optimization
      - CUDA_VISIBLE_DEVICES=all
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - CUDA_CACHE_PATH=/tmp/cuda_cache
      - TORCH_CUDA_ARCH_LIST="7.0;7.5;8.0;8.6;8.9;9.0"
      # Memory optimization (CUDA 12.4.1)
      - CUDA_MEMORY_FRACTION=${CUDA_MEMORY_FRACTION:-0.85}
      - PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
      - CUDA_MODULE_LOADING=LAZY
      - TORCH_CUDNN_V8_API_ENABLED=1
      # Numba optimization
      - NUMBA_DISABLE_JIT=0
      - NUMBA_CACHE_DIR=/tmp/numba_cache
      - NUMBA_THREADING_LAYER=workqueue
      - NUMBA_PARALLEL=1
    networks:
      - whisper-network
    depends_on:
      - redis
    # GPU configuration for Docker Compose (CUDA 12.4.1)
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
        limits:
          memory: 12G
    # Runtime configuration for older Docker versions
    runtime: nvidia
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 20s
      retries: 5
      start_period: 180s

  # Redis for caching and session storage (optimized for GPU workload)
  redis:
    image: redis:7.2-alpine
    restart: unless-stopped
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
      - ./config/redis.conf:/usr/local/etc/redis/redis.conf:ro
    command: redis-server /usr/local/etc/redis/redis.conf
    networks:
      - whisper-network
    deploy:
      resources:
        limits:
          memory: 2G
        reservations:
          memory: 512M
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 3

  # GPU monitoring (optional)
  nvidia-smi-exporter:
    image: mindprince/nvidia_gpu_prometheus_exporter:0.1
    restart: unless-stopped
    ports:
      - "9835:9835"
    volumes:
      - /usr/lib/x86_64-linux-gnu/libnvidia-ml.so:/usr/lib/x86_64-linux-gnu/libnvidia-ml.so:ro
      - /usr/lib/x86_64-linux-gnu/libnvidia-ml.so.1:/usr/lib/x86_64-linux-gnu/libnvidia-ml.so.1:ro
    networks:
      - whisper-network
    profiles:
      - monitoring
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # Prometheus for monitoring (optional)
  prometheus:
    image: prom/prometheus:v2.49.0
    restart: unless-stopped
    ports:
      - "9090:9090"
    volumes:
      - ./config/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=200h'
      - '--web.enable-lifecycle'
    networks:
      - whisper-network
    profiles:
      - monitoring

  # Grafana for visualization (optional)
  grafana:
    image: grafana/grafana:10.3.0
    restart: unless-stopped
    ports:
      - "3000:3000"
    volumes:
      - grafana_data:/var/lib/grafana
      - ./config/grafana/dashboards:/etc/grafana/provisioning/dashboards:ro
      - ./config/grafana/datasources:/etc/grafana/provisioning/datasources:ro
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_INSTALL_PLUGINS=grafana-clock-panel,grafana-simple-json-datasource
    networks:
      - whisper-network
    depends_on:
      - prometheus
    profiles:
      - monitoring

volumes:
  redis_data:
    driver: local
  prometheus_data:
    driver: local
  grafana_data:
    driver: local
  gpu_model_cache:
    driver: local
  gpu_huggingface_cache:
    driver: local

networks:
  whisper-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16